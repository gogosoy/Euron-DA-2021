{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 앙상블 학습과 랜덤 포레스트\n",
    "\n",
    "일련의 예측기를 `앙상블`이라 부르며, 이를 `앙상블 학습`이라고 함. 예를 들어 결정 트리의 앙상블을 랜덤 포레스트라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 투표 기반 분류기\n",
    "\n",
    "- 각 분류기의 예측을 모아 가장 많이 선택된 클래스를 예측, `직접 투표` 분류기\n",
    "- 약한 학습기도 모이면 강한 학습기가 되는데, 이는 큰 수의 법칙으로 설명할 수 있음\n",
    "- 앙상블 방법은 **예측기가 독립적일 떄 최고의 성능**을 발휘\n",
    "- 다양한 분류기를 얻는 방법은 **각기 다른 알고리즘으로 학습**시키는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "투표 기반 분류기(VotingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`voting = hard/soft`**\n",
    "\n",
    "`hard`: 직접 투표  \n",
    "`soft`: 간접 투표  \n",
    "\n",
    "- 분류기가 predict_proba() 메서드가 있으면 예측을 평균내어 확률이 가장 높은 클래스를 예측\n",
    "- 직접 투표 방식보다 성능이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 배깅과 페이스팅\n",
    "\n",
    "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbCmIoE%2Fbtqy0RW3DBW%2Fcu6uK7BXlJ5KBUZSmz7R0K%2Fimg.png' width=70%></img>\n",
    "\n",
    "**같은 알고리즘을 사용하되 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습**  \n",
    "**`배깅`**: 중복 허용하여 샘플링  \n",
    "**`페이스팅`**: 중복 허용 X  \n",
    "  \n",
    "- 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있음\n",
    "- 수집 함수; 분류-최빈값(hard voting), 회귀-평균\n",
    "- CPU 코어나 서버에서병렬로 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 사이킷런의 배깅과 페이스팅\n",
    "\n",
    "- `bootstrap=False`: 페이스팅 사용\n",
    "- `n_jobs`: 사용할 CPU 코어 수, -1로 지정하면 모든 코어 사용(디폴트 1)\n",
    "- 배깅은 서브셋의 다양성을 증가시키고, 배깅이 페이스팅보다 더 나은 모델을 만들기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결정 트리 분류기 500개의 앙상블 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    max_samples = 100, bootstrap = True, n_jobs = -1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 oob 평가\n",
    "\n",
    "- **oob**: 선택되지 않은 훈련 샘플  \n",
    "- 앙상블 평가는 각 샘플의 obb 평가를 평균하여 얻음\n",
    "- `oob_score = True`를 배깅에서사용하여 obb 평가를 수행할 수 있음\n",
    "- `oob_score_` 평가 점수 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    bootstrap = True, n_jobs = -1, oob_score = True\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비슷한 정확도를 가지는 것 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`oob_decision_function_`: 각 훈련 샘플의 클래스 확률 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36021505, 0.63978495],\n",
       "       [0.26203209, 0.73796791],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04046243, 0.95953757],\n",
       "       [0.32432432, 0.67567568],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.98984772, 0.01015228],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [0.7740113 , 0.2259887 ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.82125604, 0.17874396],\n",
       "       [0.87209302, 0.12790698],\n",
       "       [0.95906433, 0.04093567],\n",
       "       [0.05617978, 0.94382022],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [0.96610169, 0.03389831],\n",
       "       [0.95783133, 0.04216867],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [0.37356322, 0.62643678],\n",
       "       [0.91304348, 0.08695652],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98958333, 0.01041667],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.59459459, 0.40540541],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15135135, 0.84864865],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.34482759, 0.65517241],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21287129, 0.78712871],\n",
       "       [0.35602094, 0.64397906],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01530612, 0.98469388],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98404255, 0.01595745],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [0.97938144, 0.02061856],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05263158, 0.94736842],\n",
       "       [0.98709677, 0.01290323],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [0.99061033, 0.00938967],\n",
       "       [0.73770492, 0.26229508],\n",
       "       [0.40883978, 0.59116022],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65      , 0.35      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.8579235 , 0.1420765 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.65340909, 0.34659091],\n",
       "       [0.08928571, 0.91071429],\n",
       "       [0.67955801, 0.32044199],\n",
       "       [0.94680851, 0.05319149],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15882353, 0.84117647],\n",
       "       [0.91111111, 0.08888889],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04123711, 0.95876289],\n",
       "       [0.01754386, 0.98245614],\n",
       "       [0.25806452, 0.74193548],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84210526, 0.15789474],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.28571429, 0.71428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95135135, 0.04864865],\n",
       "       [0.76536313, 0.23463687],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24120603, 0.75879397],\n",
       "       [0.625     , 0.375     ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02234637, 0.97765363],\n",
       "       [0.43684211, 0.56315789],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23863636, 0.76136364],\n",
       "       [0.49197861, 0.50802139],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03626943, 0.96373057],\n",
       "       [0.99342105, 0.00657895],\n",
       "       [0.31578947, 0.68421053],\n",
       "       [0.86440678, 0.13559322],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82802548, 0.17197452],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96703297, 0.03296703],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93548387, 0.06451613],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02139037, 0.97860963],\n",
       "       [0.18128655, 0.81871345],\n",
       "       [0.96195652, 0.03804348],\n",
       "       [0.27419355, 0.72580645],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [0.69354839, 0.30645161],\n",
       "       [0.35465116, 0.64534884],\n",
       "       [0.38216561, 0.61783439],\n",
       "       [0.86243386, 0.13756614],\n",
       "       [0.92146597, 0.07853403],\n",
       "       [0.03804348, 0.96195652],\n",
       "       [0.79768786, 0.20231214],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02808989, 0.97191011],\n",
       "       [0.98843931, 0.01156069],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [0.        , 1.        ],\n",
       "       [0.34090909, 0.65909091],\n",
       "       [0.2893401 , 0.7106599 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.26589595, 0.73410405],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02094241, 0.97905759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.67759563, 0.32240437],\n",
       "       [0.91623037, 0.08376963],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.99489796, 0.00510204],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.08125   , 0.91875   ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03465347, 0.96534653],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01052632, 0.98947368],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93181818, 0.06818182],\n",
       "       [0.71052632, 0.28947368],\n",
       "       [0.60773481, 0.39226519],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13227513, 0.86772487],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95212766, 0.04787234],\n",
       "       [0.98857143, 0.01142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42180095, 0.57819905],\n",
       "       [0.89772727, 0.10227273],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00985222, 0.99014778],\n",
       "       [0.0060241 , 0.9939759 ],\n",
       "       [0.97252747, 0.02747253],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27659574, 0.72340426],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.83636364, 0.16363636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03921569, 0.96078431],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04040404, 0.95959596],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [0.99484536, 0.00515464],\n",
       "       [0.78172589, 0.21827411],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.89      , 0.11      ],\n",
       "       [0.99481865, 0.00518135],\n",
       "       [0.23157895, 0.76842105],\n",
       "       [0.24064171, 0.75935829],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24064171, 0.75935829],\n",
       "       [0.96410256, 0.03589744],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48469388, 0.51530612],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05913978, 0.94086022],\n",
       "       [0.09569378, 0.90430622],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.39459459, 0.60540541],\n",
       "       [0.11464968, 0.88535032],\n",
       "       [0.53107345, 0.46892655],\n",
       "       [0.6       , 0.4       ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61585366, 0.38414634],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20833333, 0.79166667],\n",
       "       [0.81868132, 0.18131868],\n",
       "       [0.06358382, 0.93641618],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83854167, 0.16145833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01162791, 0.98837209],\n",
       "       [0.10280374, 0.89719626],\n",
       "       [0.01666667, 0.98333333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86390533, 0.13609467],\n",
       "       [0.12903226, 0.87096774],\n",
       "       [0.96517413, 0.03482587],\n",
       "       [0.        , 1.        ],\n",
       "       [0.57653061, 0.42346939],\n",
       "       [0.06593407, 0.93406593],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83030303, 0.16969697],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29775281, 0.70224719],\n",
       "       [0.99514563, 0.00485437],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02298851, 0.97701149],\n",
       "       [0.90710383, 0.09289617],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.80748663, 0.19251337],\n",
       "       [0.91256831, 0.08743169],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74074074, 0.25925926],\n",
       "       [0.54304636, 0.45695364],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89240506, 0.10759494],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.85106383, 0.14893617],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.73780488, 0.26219512],\n",
       "       [0.09770115, 0.90229885],\n",
       "       [0.52147239, 0.47852761],\n",
       "       [0.24117647, 0.75882353],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87845304, 0.12154696],\n",
       "       [0.86666667, 0.13333333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02409639, 0.97590361],\n",
       "       [0.96932515, 0.03067485],\n",
       "       [0.96335079, 0.03664921],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48543689, 0.51456311],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0104712 , 0.9895288 ],\n",
       "       [0.97979798, 0.02020202],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96391753, 0.03608247],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07978723, 0.92021277],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.1185567 , 0.8814433 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33510638, 0.66489362],\n",
       "       [0.08247423, 0.91752577],\n",
       "       [0.22613065, 0.77386935],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [0.24456522, 0.75543478],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96464646, 0.03535354],\n",
       "       [0.4047619 , 0.5952381 ],\n",
       "       [0.98314607, 0.01685393],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98395722, 0.01604278],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01764706, 0.98235294],\n",
       "       [0.97175141, 0.02824859],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03888889, 0.96111111],\n",
       "       [0.60893855, 0.39106145]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 랜덤 패치와 랜덤 서브스페이스\n",
    "\n",
    "특성 샘플링은 더 다양한 예측기를 만들고 편향을 늘리는 대신 분산을 낮춤\n",
    "- `랜덤 패치 방식`: 훈련 특성/샘플 모두 샘플링\n",
    "- `서브스페이스 방식`: 훈련 샘플을모두 사용하고 특성은 샘플링\n",
    "\n",
    "`max_features`, `bootstrap_features` 매개변수로 조절  \n",
    "샘플이 아닌 특성에 대한 샘플링  \n",
    "고차원의 데이터셋을 다룰 때 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 랜덤 포레스트\n",
    "\n",
    "배깅을 적용한 결정 트리의 앙상블\n",
    "\n",
    "- `max_samples`: 훈련 세트의 크기\n",
    "- BaggingClassifier에 DecisionTreeClassifier 대신 최적화된 RandomForestClassifier 사용\n",
    "\n",
    "트리의 노드를 분할할 때 최선의 특성을 찾는 대신 무작위 선택 후보 중 최적의 특성을 찾음  \n",
    "편향을 손해보는 대신 분산을 낮춤  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배깅분류를 사용하여 랜덤포레스트분류를 유사하게 만든 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features = 'auto', max_leaf_nodes = 16),\n",
    "    n_estimators = 500, max_samples = 1.0, bootstrap = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 엑스트라 트리\n",
    "\n",
    "- 극단적으로 무작위한 트리의 랜덤 포레스트\n",
    "- 최적의 임곗값을 찾는 대신 후보 특성을 사용해 무작위로 분할 후 최상의 분할을 선택\n",
    "- `ExtraTreesClassifier` 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 특성 중요도\n",
    "\n",
    "- `feature_importances_`: 특성의 상대적 중요도 측정\n",
    "- 평균적으로 불순도를 얼마나 감소시키는지 확인하여 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.103308914095434\n",
      "sepal width (cm) 0.02404215631644509\n",
      "petal length (cm) 0.43007404064090343\n",
      "petal width (cm) 0.44257488894721736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 부스팅\n",
    "\n",
    "- 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 기법\n",
    "- 앞의 모델을 보완하면서 일련의 예측기 학습\n",
    "- 단점: 병렬화 할 수 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 에이다부스트\n",
    "\n",
    "- 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높여(업데이트) 이전 예측기 보완\n",
    "- 가중치 합이 가장 큰 클래스가 예측 결과\n",
    "- `AdaBoostClassifier`\n",
    "- `learning rate`: 학습률 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth = 1), n_estimators = 200,\n",
    "    algorithm = 'SAMME.R', learning_rate = 0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2 그레디언트 부스팅\n",
    "\n",
    "- 이전 예측기가 만든 **잔여 오차**에 새로운 예측기 학습\n",
    "- `learning_rate`: 각 트리의 기여 정도, 낮으면 많은 트리가 필요하지만 예측 성능 좋아짐(규제-축소)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀결정트리로 학습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X) #오차\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f8f07e81cc23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 새로운 샘플에 대한 예측\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtree_reg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_reg2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_reg3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-f8f07e81cc23>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 새로운 샘플에 대한 예측\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtree_reg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_reg2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_reg3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_new' is not defined"
     ]
    }
   ],
   "source": [
    "# 새로운 샘플에 대한 예측\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 코드와 같은 앙상블 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`staged_predict()`; 최적의 트리 수를 찾기 위한 조기 종료 기법  \n",
    "훈련의 단계에서 예측기를 순회하는 반복자(iterator)를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=110)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors)+1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth = 2, n_estimators = bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`warm_start = True` fit이 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있음  \n",
    "다음은 5번의 반복 동안 검증 오차가 향상되지 않을 시 중지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_ging_up = 0\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    \n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break #조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 그레이디언트 부스팅\n",
    "\n",
    "- `subsample`로 훈련 샘플의 비율 지정 가능\n",
    "- 편향↑ 분산↓, 훈련 속도를 높임\n",
    "- XGBoost가 유명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost는 자동 조기 종료도 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.19989\n",
      "[1]\tvalidation_0-rmse:0.14641\n",
      "[2]\tvalidation_0-rmse:0.10740\n",
      "[3]\tvalidation_0-rmse:0.08246\n",
      "[4]\tvalidation_0-rmse:0.06677\n",
      "[5]\tvalidation_0-rmse:0.05907\n",
      "[6]\tvalidation_0-rmse:0.05475\n",
      "[7]\tvalidation_0-rmse:0.05229\n",
      "[8]\tvalidation_0-rmse:0.05125\n",
      "[9]\tvalidation_0-rmse:0.05094\n",
      "[10]\tvalidation_0-rmse:0.05128\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 스태킹\n",
    "\n",
    "- 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측 수행\n",
    "- 블렌더: 마지막 예측기\n",
    "- 홀드 아웃 세트를 사용해 블렌더 학습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
